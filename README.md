# crawler
using RabbitMQ


В данной работе создан робот, собирающий публикации с сайта. 
Сначала происходит скачивание новостной страницы. Затем из нее извлекаются все ссылки и добавляются в очередь. 
Далее два потока получают ссылки из очереди, скачивают соответствующие страницы, извлекают из них текст и добавляют его в другую очередь.
Еще два потока читают текст из очереди и выводят его.
